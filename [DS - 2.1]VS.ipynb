{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1d895f-5ca8-4868-a757-1e2c7fa10520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "import plotly\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "import copy\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ddfd83-171b-44a5-b370-4ab897dd3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34aec4d-9645-42af-9267-47f21dc10c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image(\"scheme.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de84561c-d4c0-409e-81dc-00561123ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/siarhei/Programming/ML/internship/data_after_eda.csv')\n",
    "sempling = pd.read_csv('/home/siarhei/Programming/ML/internship/elapsed/final_df.csv') #after sempling(10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d456c78f-6a53-4189-8ca9-4d35754580ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([sempling,data[data.date_block_num>30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a169101-730c-48e2-a590-bdbc82c4d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train=data[data.is_train == 1]\n",
    "test=data[data.is_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edbdf40d-f359-4ebf-b478-ad5afc96870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sempling\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda4906-4ea7-4573-9a9d-e5d2f6a29ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import time\n",
    "\n",
    "#worked version, just calculated from PC\n",
    "def sempling(matrix, num_of_new_row):\n",
    "    start_time = time.time()\n",
    "    filtered_df = matrix[(matrix['shop_id'].isin(matrix['shop_id'].unique())) & (matrix['item_id'].isin(matrix['item_id'].unique()))]\n",
    "    grouped = filtered_df.groupby(['shop_id','item_id'])['date_block_num'].min()\n",
    "    all_date_block_nums = matrix.date_block_num.unique()\n",
    "    for shop_id, item_id in itertools.product(matrix['shop_id'].unique(), matrix['item_id'].unique()):\n",
    "        try: \n",
    "            first_date_block_num = grouped[shop_id, item_id] \n",
    "            block_num = np.random.choice(np.extract(all_date_block_nums > first_date_block_num, all_date_block_nums))\n",
    "        except: continue\n",
    "        if num_of_new_row == 0 :\n",
    "            return matrix, grouped\n",
    "        latest_row = filtered_df[(filtered_df['shop_id'] == shop_id) & (filtered_df['item_id'] == item_id) & (filtered_df['date_block_num'] < block_num)].sort_values(by='date_block_num', ascending=False).tail(1)\n",
    "        new_row = {'date_block_num': block_num,\n",
    "        'shop_id': shop_id,\n",
    "        'item_id': item_id,\n",
    "        'item_price': latest_row['item_price'].values[0],\n",
    "        'item_cnt_day': 0.0,\n",
    "        'is_train': latest_row['is_train'].values[0],\n",
    "        'city_code': latest_row['city_code'].values[0],\n",
    "        'shop_type_code': latest_row['shop_type_code'].values[0],\n",
    "        'item_category_id': latest_row['item_category_id'].values[0],\n",
    "        'platform_id': latest_row['platform_id'].values[0],\n",
    "        'supercategory_id': latest_row['supercategory_id'].values[0],\n",
    "        'item_name_group': latest_row['item_name_group'].values[0],\n",
    "        'monthly_sales': 0.0}\n",
    "        matrix = matrix.append(new_row, ignore_index=True)\n",
    "        num_of_new_row = num_of_new_row - 1\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "    return matrix, grouped\n",
    "\n",
    "start_time = time.time()\n",
    "df_for_task = copy.deepcopy(b[b.date_block_num<=30])\n",
    "final_df, ba = sempling(df_for_task, 350000)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time : \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cee73f91-3a57-4d41-a516-fce46c4f0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_extraction:\n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.matrix = []\n",
    "    \n",
    "    def preprocess(self):\n",
    "        from itertools import product\n",
    "        cols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\n",
    "        for i in range(34):\n",
    "            sales = self.train[self.train.date_block_num == i]\n",
    "            self.matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n",
    "        \n",
    "        self.matrix = pd.DataFrame( np.vstack(self.matrix), columns = cols )\n",
    "        self.matrix[\"date_block_num\"] = self.matrix[\"date_block_num\"].astype(np.int8)\n",
    "        self.matrix[\"shop_id\"] = self.matrix[\"shop_id\"].astype(np.int8)\n",
    "        self.matrix[\"item_id\"] = self.matrix[\"item_id\"].astype(np.int16)\n",
    "        self.matrix.sort_values( cols, inplace = True )\n",
    "    \n",
    "    def add_revenue(self, cols):\n",
    "        self.train[\"revenue\"] = self.train[\"item_cnt_day\"] * self.train[\"item_price\"]\n",
    "        group = self.train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\n",
    "        group.columns = [\"item_cnt_month\"]\n",
    "        group.reset_index( inplace = True)\n",
    "        self.matrix = pd.merge( self.matrix, group, on = cols, how = \"left\" )\n",
    "        self.matrix[\"item_cnt_month\"] = self.matrix[\"item_cnt_month\"].fillna(0).astype(np.float16)\n",
    "    \n",
    "    def reduce_test_memory(self):\n",
    "        self.test[\"date_block_num\"] = self.test[\"date_block_num\"].astype(np.int8)\n",
    "        self.test[\"shop_id\"] = self.test.shop_id.astype(np.int8)\n",
    "        self.test[\"item_id\"] = self.test.item_id.astype(np.int16)\n",
    "    \n",
    "    def concatenate_to_mx(self, cols):\n",
    "        self.matrix = pd.concat([self.matrix, self.test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\n",
    "        self.matrix.fillna( 0, inplace = True )\n",
    "    \n",
    "    def lag_feature(self,lags, cols):\n",
    "        for col in cols:\n",
    "            print(col)\n",
    "            tmp = self.matrix[[\"date_block_num\", \"shop_id\",\"item_id\",col]]\n",
    "            for i in lags:\n",
    "                shifted = tmp.copy()\n",
    "                shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n",
    "                shifted.date_block_num = shifted.date_block_num + i\n",
    "                self.matrix = pd.merge(self.matrix, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    \n",
    "    def add_global_item_age(self):  \n",
    "        oldest_date_year_month = self.matrix.groupby('item_id')['date_block_num'].min()\n",
    "        merged_df = pd.merge(self.matrix, oldest_date_year_month, on='item_id')\n",
    "        merged_df = merged_df.rename(columns={'date_block_num_y': 'oldest_date'})\n",
    "        merged_df['item_age'] = merged_df['date_block_num_x'] - merged_df['oldest_date']\n",
    "        merged_df.drop(columns=['oldest_date'], inplace=True)\n",
    "        self.matrix = copy.deepcopy(merged_df)\n",
    "        self.matrix.rename(columns={'date_block_num_x':'date_block_num'}, inplace=True)\n",
    "    \n",
    "    def add_avg_sales(self, nan_values=0.0):\n",
    "        self.matrix['average_prev_sales'] = np.nan\n",
    "        date_block_nums = self.matrix['date_block_num'].unique()\n",
    "\n",
    "        for date_block_num in date_block_nums:\n",
    "            if date_block_num == 0:\n",
    "                prev_sales = self.matrix[(self.matrix['date_block_num'] == date_block_num)&(self.matrix['item_cnt_month']!=0.0)]\n",
    "                prev_sales = prev_sales.groupby('item_id')['item_cnt_day'].sum()/(date_block_num+1)\n",
    "                self.matrix.loc[self.matrix['date_block_num'] == date_block_num,'average_prev_sales'] = nan_values\n",
    "                continue\n",
    "            prev_sales = self.matrix[(self.matrix['date_block_num'] < date_block_num)&(self.matrix['item_cnt_month']!=0.0)]\n",
    "            prev_sales = prev_sales.groupby('item_id')['item_cnt_day'].sum()/(date_block_num)\n",
    "            self.matrix.loc[self.matrix['date_block_num'] == date_block_num,'average_prev_sales'] = self.matrix.loc[self.matrix['date_block_num'] == date_block_num,'item_id'].map(prev_sales)\n",
    "        self.matrix.average_prev_sales.fillna(nan_values, inplace=True)\n",
    "    \n",
    "    def add_shop_age(self):\n",
    "        min_date_block_num = self.matrix.groupby('shop_id')['date_block_num'].min()\n",
    "        self.matrix = pd.merge(self.matrix, min_date_block_num, on='shop_id', how='left', suffixes=('', '_min'))\n",
    "        self.matrix['shop_age_in_months'] = self.matrix['date_block_num'] - self.matrix['date_block_num_min']\n",
    "        self.matrix.drop(columns=['date_block_num_min'], inplace=True)\n",
    "        \n",
    "    def add_city_codes(self, shops):\n",
    "        #name fix\n",
    "        shops.loc[\n",
    "            shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"\n",
    "        ] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "        \n",
    "        \n",
    "        shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n",
    "        shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n",
    "        shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n",
    "        shop_labels = shops[[\"shop_id\", \"city_code\"]]\n",
    "        self.matrix = self.matrix.merge(shop_labels, on='shop_id', how='left')\n",
    "    \n",
    "    def add_shop_type(self, shops):    \n",
    "        #add shop_type as new column (str) *can better\n",
    "        shops['shop_type'] = shops.apply(lambda row: 'ТЦ' if row['shop_name'].find('ТЦ')>-1\\\n",
    "                                else 'ТРЦ' if row['shop_name'].find('ТРЦ')>-1\\\n",
    "                                else 'ТРК' if row['shop_name'].find('ТРК')>-1\\\n",
    "                                else 'МТРЦ' if row['shop_name'].find('МТРЦ')>-1\\\n",
    "                                else 'ТК' if row['shop_name'].find('ТК')>-1\\\n",
    "                                else 'Интернет' if row['shop_name'].find('Интернет')>-1\\\n",
    "                                else 'Склад' if row['shop_name'].find('склад')>-1\\\n",
    "                                else 'Магаз', axis =1)\n",
    "    \n",
    "        #str -> int (mb int 16 and etc)\n",
    "        shops[\"shop_type_code\"] = shops[\"shop_type\"].factorize()[0]\n",
    "        shop_labels = shops[[\"shop_id\", \"shop_type_code\"]]\n",
    "        self.matrix = self.matrix.merge(shop_labels, on='shop_id', how='left')\n",
    "\n",
    "    def add_cats(self,items):\n",
    "        #from First Place Solution Kaggle Predict Future Sales\n",
    "        self.matrix = self.matrix.merge(items[['item_id', 'item_category_id']], on='item_id', how='left')\n",
    "        platform_map = {\n",
    "            0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 8, 10: 1, 11: 2,\n",
    "            12: 3, 13: 4, 14: 5, 15: 6, 16: 7, 17: 8, 18: 1, 19: 2, 20: 3, 21: 4, 22: 5,\n",
    "            23: 6, 24: 7, 25: 8, 26: 9, 27: 10, 28: 0, 29: 0, 30: 0, 31: 0, 32: 8, 33: 11,\n",
    "            34: 11, 35: 3, 36: 0, 37: 12, 38: 12, 39: 12, 40: 13, 41: 13, 42: 14, 43: 15,\n",
    "            44: 15, 45: 15, 46: 14, 47: 14, 48: 14, 49: 14, 50: 14, 51: 14, 52: 14, 53: 14,\n",
    "            54: 8, 55: 16, 56: 16, 57: 17, 58: 18, 59: 13, 60: 16, 61: 8, 62: 8, 63: 8, 64: 8,\n",
    "            65: 8, 66: 8, 67: 8, 68: 8, 69: 8, 70: 8, 71: 8, 72: 8, 73: 0, 74: 10, 75: 0,\n",
    "            76: 0, 77: 0, 78: 0, 79: 8, 80: 8, 81: 8, 82: 8, 83: 8,\n",
    "        }\n",
    "        self.matrix['platform_id'] = self.matrix['item_category_id_x'].map(platform_map)\n",
    "        \n",
    "        supercat_map = {\n",
    "            0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 1, 11: 1, 12: 1,\n",
    "            13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 3, 21: 3, 22: 3, 23: 3,\n",
    "            24: 3, 25: 0, 26: 2, 27: 3, 28: 3, 29: 3, 30: 3, 31: 3, 32: 2, 33: 2, 34: 2,\n",
    "            35: 2, 36: 2, 37: 4, 38: 4, 39: 4, 40: 4, 41: 4, 42: 5, 43: 5, 44: 5, 45: 5,\n",
    "            46: 5, 47: 5, 48: 5, 49: 5, 50: 5, 51: 5, 52: 5, 53: 5, 54: 5, 55: 6, 56: 6,\n",
    "            57: 6, 58: 6, 59: 6, 60: 6, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0,\n",
    "            68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 7, 74: 7, 75: 7, 76: 7, 77: 7, 78: 7,\n",
    "            79: 2, 80: 2, 81: 0, 82: 0, 83: 0\n",
    "        }\n",
    "        self.matrix['supercategory_id'] = self.matrix['item_category_id_x'].map(supercat_map)\n",
    "\n",
    "    \n",
    "    def add_item_name_groups(self, items, sim_thresh, feature_name=\"item_name_group\"):\n",
    "        import re\n",
    "        from fuzzywuzzy import fuzz\n",
    "        def partialmatchgroups(items, sim_thresh=sim_thresh):\n",
    "            def strip_brackets(string):\n",
    "                string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "                string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "                return string\n",
    "    \n",
    "            items = items.copy()\n",
    "            items[\"nc\"] = items.item_name.apply(strip_brackets)\n",
    "            items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n",
    "    \n",
    "            def partialcompare(s):\n",
    "                return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n",
    "    \n",
    "            items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n",
    "            # Assign groups\n",
    "            grp = 0\n",
    "            for i in range(items.shape[0]):\n",
    "                items.loc[i, \"partialmatchgroup\"] = grp\n",
    "                if items.loc[i, \"partialmatch\"] < sim_thresh:\n",
    "                    grp += 1\n",
    "            items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n",
    "            return items\n",
    "    \n",
    "        items = partialmatchgroups(items)\n",
    "        items = items.rename(columns={\"partialmatchgroup\": feature_name})\n",
    "        items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n",
    "    \n",
    "        items[feature_name] = items[feature_name].apply(str)\n",
    "        items[feature_name] = items[feature_name].factorize()[0]\n",
    "        self.matrix = self.matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "    \n",
    "    #mb add float and etc\n",
    "    def reduce_memory_usage(self):\n",
    "        numeric_cols = self.matrix.select_dtypes(include=['float64', 'int64'])\n",
    "        for i in numeric_cols:\n",
    "            max_value = self.matrix[i].max()\n",
    "            if max_value < 127:\n",
    "                self.matrix[i] = self.matrix[i].astype(np.int8)\n",
    "            elif max_value < 32767:\n",
    "                self.matrix[i] = self.matrix[i].astype(np.int16)\n",
    "            elif max_value < 2147483647:\n",
    "                self.matrix[i] = self.matrix[i].astype(np.int32)\n",
    "            else:\n",
    "                self.matrix[i] = self.matrix[i].astype(np.int64)\n",
    "            \n",
    "            \n",
    "    def sort_data(self, column_name):\n",
    "        self.matrix.sort_values(by=column_name, inplace=True)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "023b83b0-1649-4f1b-9474-bf2c3dcf5163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_month\n",
      "additional features\n"
     ]
    }
   ],
   "source": [
    "FE = Feature_extraction(train, test)\n",
    "\n",
    "FE.preprocess()\n",
    "FE.add_revenue(cols=[\"date_block_num\", \"shop_id\", \"item_id\"])\n",
    "FE.reduce_test_memory()\n",
    "FE.concatenate_to_mx(cols=[\"date_block_num\", \"shop_id\", \"item_id\"])\n",
    "FE.lag_feature([1,2,3,6,12], [\"item_cnt_month\"])\n",
    "FE.sort_data('date_block_num')\n",
    "FE.add_shop_age()\n",
    "FE.add_avg_sales()\n",
    "FE.add_global_item_age()\n",
    "FE.sort_data('date_block_num')\n",
    "print('additional features')\n",
    "shops=pd.read_csv(\"/home/siarhei/Programming/ML/Data/Predict Future Sales/shops.csv\")\n",
    "FE.add_city_codes(shops = shops)\n",
    "FE.add_shop_type(shops = shops)\n",
    "del shops\n",
    "items=pd.read_csv(\"/home/siarhei/Programming/ML/Data/Predict Future Sales/items.csv\")\n",
    "FE.add_cats(items=items)\n",
    "FE.add_item_name_groups(items=items, sim_thresh=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ece8c5f-f0fe-4627-9015-878f68ef5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "FE.reduce_memory_usage()\n",
    "del items\n",
    "del train\n",
    "del test\n",
    "data = FE.get_data()\n",
    "del FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ed8c902-b4bb-4d6b-a3b6-7e82bc71ba15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "817.9179601669312"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(data)/1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b76519f-17fc-47c3-907a-89f3a2d78eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "class ModelValidator:\n",
    "    def __init__(self, train, dev, target_name):\n",
    "        self.train_x = train.drop([target_name], axis=1) \n",
    "        self.train_y = train[target_name]\n",
    "        \n",
    "        self.dev_x = dev.drop([target_name], axis=1)\n",
    "        self.dev_y = dev[target_name]\n",
    "        \n",
    "        self.model = None\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_dev(self):\n",
    "        return self.dev_x, self.dev_y\n",
    "    \n",
    "    def get_train(self):\n",
    "        return self.train_x, self.train_y\n",
    "\n",
    "    \n",
    "    def set_model(self, model_name):\n",
    "        if model_name == 'xgbr':\n",
    "            import gc\n",
    "            import pickle\n",
    "            from xgboost import XGBRegressor\n",
    "            from matplotlib.pylab import rcParams\n",
    "            self.model = XGBRegressor(max_depth=4,n_estimators=10)\n",
    "    \n",
    "    def train_model(self):\n",
    "        self.model.fit(self.train_x,self.train_y,eval_metric=\"rmse\")\n",
    "        y_pred = self.model.predict(self.dev_x)\n",
    "        print(f'\\nMSE : {mean_squared_error(self.dev_y, y_pred)}\\nMAE : {mean_absolute_error(self.dev_y, y_pred)}\\n R2 : {r2_score(self.dev_y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cda6d87-e063-4ca5-bef0-7210d3e78439",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "data.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89c9617e-1da8-445e-988d-5bcba29cb661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE : 22.60926055908203\n",
      "MAE : 0.377547025680542\n",
      " R2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train = data[data.date_block_num<=30]\n",
    "test = data[(data.date_block_num>30)&(data.date_block_num<=33)]\n",
    "kaggle = data[data.date_block_num==34].drop(['item_cnt_month'], axis=1)\n",
    "del data\n",
    "mv = ModelValidator(train, test, target_name='item_cnt_month')\n",
    "mv.set_model(model_name='xgbr')\n",
    "mv.train_model()\n",
    "model = mv.get_model()\n",
    "del mv\n",
    "\n",
    "Y_test = model.predict(kaggle)\n",
    "test = pd.read_csv('/home/siarhei/Programming/ML/Data/Predict Future Sales/test.csv')\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('xgb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299fd35f-8d31-433e-bc5f-8daf6f0b60f4",
   "metadata": {},
   "source": [
    "The resulting prediction gave me Score: 1.8718 on Kaggle\n",
    "\n",
    "Of course this is definitely not a good score\n",
    "\n",
    "but it shows that the data is processed correctly and the necessary structure is saved for Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
