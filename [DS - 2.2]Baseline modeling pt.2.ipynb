{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15429b24-1fb5-4980-8aed-4d79077bd5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import plotly\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "import copy\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d67c68-5aba-4832-b6d8-2cd82e9ed2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d29ba52-8eb8-4580-b324-2b4e4d6ad533",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('after_vs.csv')\n",
    "raw_data = pd.read_csv('/home/siarhei/Programming/ML/Data/Predict Future Sales/sales_train.csv')\n",
    "test = pd.read_csv('/home/siarhei/Programming/ML/Data/Predict Future Sales/test.csv')\n",
    "items = pd.read_csv('/home/siarhei/Programming/ML/time folder/from_pc/internship/Data/items.csv')\n",
    "shops = pd.read_csv('/home/siarhei/Programming/ML/time folder/from_pc/internship/Data/shops.csv')\n",
    "items_desc = pd.read_csv('/home/siarhei/Programming/ML/time folder/from_pc/internship/Data/items.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f43ef3-dc27-4d53-9db3-eb1196d4de97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46fa4e18-5f40-4e0c-acb2-2a315543fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_extraction:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def add_global_item_age(self):\n",
    "        matrix = copy.deepcopy(self.data)    \n",
    "        oldest_date_year_month = matrix.groupby('item_id')['date_block_num'].min()\n",
    "        merged_df = pd.merge(matrix, oldest_date_year_month, on='item_id')\n",
    "        merged_df = merged_df.rename(columns={'date_block_num_y': 'oldest_date'})\n",
    "        merged_df['item_age'] = merged_df['date_block_num_x'] - merged_df['oldest_date']\n",
    "        merged_df.drop(columns=['oldest_date'], inplace=True)\n",
    "        self.data = copy.deepcopy(merged_df)\n",
    "        self.data.rename(columns={'date_block_num_x':'date_block_num'}, inplace=True)\n",
    "\n",
    "    def add_sales_interval(self, fill_na=True, value = 6666.0):\n",
    "        matrix = copy.deepcopy(self.data)\n",
    "        matrix['date_block_num_diff'] = matrix.groupby('item_id')['date_block_num'].diff().apply(lambda x: x-1 if x == 1 else x)\n",
    "        if fill_na:\n",
    "            idx = matrix['date_block_num_diff'].isnull( )\n",
    "            matrix['date_block_num_diff'][ idx ] = value\n",
    "        self.data = copy.deepcopy(matrix)\n",
    "    \n",
    "    def add_avg_sales(self, nan_values=0.0):\n",
    "        df = copy.deepcopy(self.data)\n",
    "        df['average_prev_sales'] = np.nan\n",
    "        date_block_nums = df['date_block_num'].unique()\n",
    "\n",
    "        for date_block_num in date_block_nums:\n",
    "            if date_block_num == 0:\n",
    "                prev_sales = df[df['date_block_num'] == date_block_num]\n",
    "                prev_sales = prev_sales.groupby('item_id')['item_cnt_day'].sum()/(date_block_num+1)\n",
    "                df.loc[df['date_block_num'] == date_block_num,'average_prev_sales'] = nan_values\n",
    "                continue\n",
    "            prev_sales = df[df['date_block_num'] < date_block_num]\n",
    "            prev_sales = prev_sales.groupby('item_id')['item_cnt_day'].sum()/(date_block_num)\n",
    "            df.loc[df['date_block_num'] == date_block_num,'average_prev_sales'] = df.loc[df['date_block_num'] == date_block_num,'item_id'].map(prev_sales)\n",
    "        df.fillna(nan_values, inplace=True)\n",
    "        self.data = copy.deepcopy(df)\n",
    "    \n",
    "    def add_shop_age(self):\n",
    "        matrix = copy.deepcopy(self.data)\n",
    "        min_date_block_num = matrix.groupby('shop_id')['date_block_num'].min()\n",
    "        matrix = pd.merge(matrix, min_date_block_num, on='shop_id', how='left', suffixes=('', '_min'))\n",
    "        matrix['shop_age_in_months'] = matrix['date_block_num'] - matrix['date_block_num_min']\n",
    "        matrix.drop(columns=['date_block_num_min'], inplace=True)\n",
    "        self.data = copy.deepcopy(matrix)\n",
    "    \n",
    "    def add_store_interval(self, fill_na=True, value = 9999.9):\n",
    "        matrix = copy.deepcopy(self.data)\n",
    "        matrix = matrix.sort_values(by=['shop_id','date_block_num','date'])\n",
    "        matrix['month_from_prev_sale'] = matrix.groupby('shop_id')['date_block_num'].diff()\n",
    "        \n",
    "        if fill_na:\n",
    "            idx = matrix['month_from_prev_sale'].isnull( )\n",
    "            matrix['month_from_prev_sale'][ idx ] = value\n",
    "        self.data = copy.deepcopy(matrix)\n",
    "    \n",
    "    def add_monthly_sales(self):\n",
    "        matrix = copy.deepcopy(self.data)\n",
    "        matrix['monthly_sales'] = matrix.groupby(['date_block_num','shop_id','item_id'])['item_cnt_day'].transform('sum')\n",
    "        self.data = copy.deepcopy(matrix)\n",
    "    \n",
    "    def sort_data(self, column_name):\n",
    "        self.data.sort_values(by=column_name, inplace=True)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "#Check DS 2.1 VS, there was preprocessing\n",
    "\n",
    "#a = copy.deepcopy(data)\n",
    "#FE = Feature_extraction(a)\n",
    "#FE.sort_data('date_block_num')\n",
    "#FE.add_sales_interval()\n",
    "#FE.add_monthly_sales()\n",
    "#FE.add_shop_age()\n",
    "#FE.add_store_interval()\n",
    "#FE.add_avg_sales()\n",
    "#FE.add_global_item_age()\n",
    "#FE.sort_data('date_block_num')\n",
    "#df = FE.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78afbdf-b14d-4543-ab00-bdb7bbb64a28",
   "metadata": {},
   "source": [
    "## ModelValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08d252d6-4dc1-49f2-bb43-bb9e4fea67a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class ModelValidator:\n",
    "    def __init__(self, data, target_name, param_grid):\n",
    "        \n",
    "        self.model = None\n",
    "        self.full_data = data\n",
    "        self.target_data = data[[target_name]]\n",
    "        self.target_name = target_name\n",
    "        #Get data without target column\n",
    "        self.data = data.drop(target_name, axis = 1)\n",
    "        self.tscv = None\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        self.param_grid = param_grid\n",
    "        self.best_models = {}\n",
    "        self.best_params_models = {}\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_test(self):\n",
    "        return self.test_x, self.test_y\n",
    "    \n",
    "    def get_train(self):\n",
    "        return self.train_x, self.train_y\n",
    "\n",
    "    def create_k_folds(self, data, n_splits=33):\n",
    "        unique_date_block_nums = data['date_block_num'].unique()\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        for train_index, test_index in kf.split(unique_date_block_nums):\n",
    "            train_date_block_nums = unique_date_block_nums[train_index]\n",
    "            test_date_block_nums = unique_date_block_nums[test_index]\n",
    "            train_data = data[data['date_block_num'].isin(train_date_block_nums)]\n",
    "            test_data = data[data['date_block_num'].isin(test_date_block_nums)]\n",
    "            yield (train_data, test_data)\n",
    "    \n",
    "    def grid_search_with_date_block_num_k_folds(self, models, param_grid, n_splits=33):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "        self.best_params_models = {}\n",
    "    \n",
    "        # Iterate through the models\n",
    "        for model in models.keys():\n",
    "            for train_data, test_data in self.create_k_folds(self.full_data, n_splits):\n",
    "                print(train_data.shape[0])\n",
    "                # Initialize GridSearchCV with the model, param_grid, cv, and return_train_score\n",
    "                grid_search = GridSearchCV(models[model], param_grid[model], return_train_score=True)\n",
    "                # Fit the data to the grid search\n",
    "                grid_search.fit(train_data.drop('monthly_sales', axis=1), train_data['monthly_sales'])\n",
    "    \n",
    "                # Print the results of the grid search\n",
    "                print(f'Model: {model}')\n",
    "                print(\"Best score: \", grid_search.best_score_)\n",
    "                print(\"Best parameters: \", grid_search.best_params_)\n",
    "                print(\"Test Score: \", grid_search.score(test_data.drop('monthly_sales', axis=1), test_data['monthly_sales']))\n",
    "                print(\"=\"*50)\n",
    "    \n",
    "                # Save the best parameters and best model to the dictionary\n",
    "                self.best_params_models[model] = (grid_search.best_params_, grid_search.best_estimator_)\n",
    "\n",
    "\n",
    "    \n",
    "    def grid_search_with_time_series_split(self, models, param_grid):\n",
    "            from sklearn.model_selection import TimeSeriesSplit\n",
    "            from sklearn.model_selection import GridSearchCV\n",
    "            \n",
    "            # Set the index of the dataframe to the date_block_num column\n",
    "            self.full_data = self.full_data.set_index('date_block_num')\n",
    "            \n",
    "            # Initialize TimeSeriesSplit with the number of splits (33)\n",
    "            self.tscv = TimeSeriesSplit(n_splits=33)\n",
    "            \n",
    "            # Initialize a dictionary to store the best parameters and models\n",
    "            self.best_params_models = {}\n",
    "            \n",
    "            # Iterate through the models\n",
    "            for model in models.keys():\n",
    "                # Initialize GridSearchCV with the model, param_grid, cv, and return_train_score\n",
    "                grid_search = GridSearchCV(models[model], param_grid[model], cv=self.tscv, return_train_score=True)\n",
    "                \n",
    "                # Fit the data to the grid search\n",
    "                grid_search.fit(self.full_data.drop('monthly_sales', axis=1), self.full_data['monthly_sales'])\n",
    "                \n",
    "                print(f'Model: {model}')\n",
    "                print(\"Best score: \", grid_search.best_score_)\n",
    "                print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "                \n",
    "                # Save the best parameters and best model to the dictionary\n",
    "                self.best_params_models[model] = (grid_search.best_params_, grid_search.best_estimator_)\n",
    "\n",
    "    def train_model(self):\n",
    "        for i in range(len(self.train_x)):\n",
    "            self.model.fit(self.train_x[i], self.train_y[i])\n",
    "            y_pred = self.model.predict(self.test_x[i])\n",
    "            print(f'Batch : {i}')\n",
    "            if i >=32:\n",
    "                print(f'Batch : {i}\\nMSE : {mean_squared_error(self.test_y[i], y_pred)}\\nMAE : {mean_absolute_error(self.test_y[i], y_pred)}\\n R2 : {r2_score(self.test_y[i], y_pred)}')\n",
    "    \n",
    "    def split_data(self, n_splits=33):\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        # Create a TimeSeriesSplit object with the number of splits you want\n",
    "        self.tscv = TimeSeriesSplit(n_splits=33)\n",
    "        \n",
    "        # Get the index of the dataframe\n",
    "        self.full_data = self.full_data.set_index('date_block_num')\n",
    "        \n",
    "        # Iterate through the splits\n",
    "        for train_index, test_index in self.tscv.split(self.full_data):\n",
    "            # Get the train and test data for the current iteration\n",
    "            self.train_x, self.train_y = self.full_data.iloc[train_index, :], self.full_data['monthly_sales'].iloc[train_index]\n",
    "            print(self.train_x.shape)\n",
    "            self.test_x, self.test_y = self.full_data.iloc[test_index, :], self.full_data['monthly_sales'].iloc[test_index]\n",
    "            # Train and test your model on the data\n",
    "\n",
    "    def train_sliding_windows(self, window_size=7, step=1):\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "        \n",
    "        # Set the index of the dataframe to the date_block_num column\n",
    "        self.full_data['date_block_num_x'] = self.full_data['date_block_num']\n",
    "        self.full_data = self.full_data.set_index('date_block_num_x')\n",
    "        \n",
    "        # Initialize starting index\n",
    "        start_index = 0\n",
    "        end_index = window_size\n",
    "    \n",
    "        while end_index <= self.full_data.index.max():\n",
    "            # Get the train and test data for the current iteration\n",
    "            train_x = self.full_data.drop('monthly_sales', axis=1).loc[start_index:end_index-1]\n",
    "            train_y = self.full_data['monthly_sales'].loc[start_index:end_index-1]\n",
    "            test_x = self.full_data.drop('monthly_sales', axis=1).loc[end_index:end_index+step-1]\n",
    "            test_y = self.full_data['monthly_sales'].loc[end_index:end_index+step-1]\n",
    "            # Fit the model to the training data\n",
    "            self.model.fit(train_x, train_y)\n",
    "            \n",
    "            # Make predictions on the test data\n",
    "            y_pred = self.model.predict(test_x)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f'\\nMSE : {mean_squared_error(test_y, y_pred)}\\nMAE : {mean_absolute_error(test_y, y_pred)}\\nR2 : {r2_score(test_y, y_pred)}\\n')\n",
    "            \n",
    "            # Update the starting index for the next iteration\n",
    "            start_index += step\n",
    "            end_index += step\n",
    "        \n",
    "    #works better than expanding\n",
    "    def fixed_split(self, months_to_train):\n",
    "        self.train_x = self.data[self.data.date_block_num <=months_to_train]\n",
    "        self.train_y = self.target_data.head(self.train_x.shape[0])\n",
    "        self.test_x = self.data[self.data.date_block_num >months_to_train]\n",
    "        self.test_y = self.target_data.tail(self.test_x.shape[0])\n",
    "    \n",
    "    #overfit to first... month\n",
    "    def expanding_windows(self, num_of_windows=33):\n",
    "        splited_by_month_x = []\n",
    "        splited_by_month_y = []\n",
    "        \n",
    "        data_blocks = self.data['date_block_num'].unique()\n",
    "        \n",
    "        for month in data_blocks:\n",
    "            df = self.data[self.data['date_block_num'].isin([month])]\n",
    "            indices = self.data.index[self.data['date_block_num'] == month]\n",
    "            \n",
    "            splited_by_month_x.append(df.values.tolist())\n",
    "            splited_by_month_y.append(self.target_data.loc[indices].values.tolist())\n",
    "        \n",
    "        self.train_x = [splited_by_month_x[0]]\n",
    "        self.train_y = [splited_by_month_y[0]]\n",
    "        \n",
    "        self.test_x = [splited_by_month_x[1]]\n",
    "        self.test_y = [splited_by_month_y[1]]\n",
    "        \n",
    "        for i in range(1, num_of_windows):\n",
    "            self.train_x.append(self.train_x[-1]+splited_by_month_x[i])\n",
    "            self.train_y.append(self.train_y[-1]+splited_by_month_y[i])\n",
    "\n",
    "            self.test_x.append(splited_by_month_x[i+1])\n",
    "            self.test_y.append(splited_by_month_y[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "320e5121-0ee9-424f-8257-28a1eb658065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2fd69d39-e8db-4f03-bbe2-6c2510ae04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all duplicates to avoid overfitting\n",
    "df = df.groupby(['date_block_num', 'shop_id', 'item_id']).last().reset_index()\n",
    "test = df[df.is_train == False]\n",
    "train = df[df['is_train']==True]\n",
    "train.drop(columns=['date','item_cnt_day', 'is_train', 'ID','Unnamed: 0'], inplace=True)\n",
    "test.drop(columns=['date','item_cnt_day', 'is_train', 'monthly_sales', 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66883984-3d60-4064-83d8-aa4239e41105",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr_lasso': {'alpha': [0.1, 0.3, 0.5, 0.7, 1.0]},\n",
    "    'lr_ridge': {'alpha': [0.1, 0.3, 0.5, 0.7, 1.0]},\n",
    "    'elastic_net': {'alpha': [0.1, 0.3, 0.5, 0.7, 1.0], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]},\n",
    "    'decision_trees': {'max_depth': [3, 5, 7, 9, 11]},\n",
    "    'rf': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9, 11], 'n_jobs': [-1]},\n",
    "    'svr': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'bayesian_lr': {'n_iter': [200, 300, 400], 'tol': [0.001, 0.01, 0.1]},\n",
    "    'xgboost': {'learning_rate': [0.1, 0.01], 'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9, 11], 'n_jobs': [-1]},\n",
    "    'lightGBM': {'learning_rate': [0.1, 0.01], 'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9, 11], 'n_jobs': [-1]},\n",
    "    'catboost': {'learning_rate': [0.1, 0.01], 'n_estimators': [50, 100, 200]},\n",
    "    'adaboost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 1]},\n",
    "}\n",
    "models = {\n",
    "    'lr_lasso': Lasso()\n",
    "    'lr_ridge': Ridge(),\n",
    "    'elastic_net': ElasticNet(),\n",
    "    'decision_trees': DecisionTreeRegressor(),\n",
    "    'rf': RandomForestRegressor(),\n",
    "    'svr': SVR(),\n",
    "    'bayesian_lr': BayesianRidge(),\n",
    "    'xgboost': xgb.XGBRegressor(),\n",
    "    'lightGBM': lgb.LGBMRegressor(),\n",
    "    'catboost': CatBoostRegressor(),\n",
    "    'adaboost': AdaBoostRegressor()\n",
    "}\n",
    "\n",
    "\n",
    "mv = ModelValidator(data=train, target_name='monthly_sales', param_grid=param_grid)\n",
    "#mv.grid_search_with_date_block_num_k_folds(models, param_grid, n_splits=33)\n",
    "mv.grid_search_with_time_series_split(models, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d85aa-3e3d-4522-bf64-e7abdba0b567",
   "metadata": {},
   "source": [
    "The cell above gave me the following output\n",
    "\n",
    "    Model: lr_lasso\n",
    "    Best score:  8.17786178270079242\n",
    "    Best parameters:  {'alpha': 0.1}\n",
    "    \n",
    "    Model: lr_ridge\n",
    "    Best score:  8.17358088914778932\n",
    "    Best parameters:  {'alpha': 1.0}\n",
    "    \n",
    "    Model: elastic_net\n",
    "    Best score:  7.17784508532135762\n",
    "    Best parameters:  {'alpha': 0.1, 'l1_ratio': 0.9}\n",
    "    \n",
    "    Model: decision_trees\n",
    "    Best score:  0.31183776324220747\n",
    "    Best parameters:  {'max_depth': 5}\n",
    "    \n",
    "    Model: rf\n",
    "    Best score:  0.40362900835055154\n",
    "    Best parameters:  {'max_depth': 9, 'n_estimators': 100, 'n_jobs': -1}\n",
    "    \n",
    "    Model: catboost\n",
    "    Best score:  0.47858537853716376\n",
    "    Best parameters:  {'learning_rate': 0.1, 'n_estimators': 200}\n",
    "    \n",
    "here there was an explicit overfitting, but the search for the necessary parameters is still useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2bfdc-b329-41aa-a145-33700fa38327",
   "metadata": {},
   "source": [
    "Main work will be with CatBoost(as we agreed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085435a-3df8-4b44-a3f4-5502f42c6fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
